{"cells":[{"cell_type":"markdown","metadata":{"id":"_8OdQ0x-mG9g","colab_type":"text"},"source":["# Detect and save cropped image"]},{"cell_type":"code","metadata":{"id":"JZ23wU4Dl5-7","colab_type":"code","colab":{}},"source":["import os\n","import glob\n","import time\n","\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","CONFTHRESH = 0.5\n","NMSTHRESH = 0.3  # Need to try 0.3\n","PATH = './'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpOd5LYkmEBs","colab_type":"code","colab":{}},"source":["def get_weights(weights_path):\n","    \"\"\"\n","    Returns model.weights file path\n","    \"\"\"\n","    return os.path.sep.join([PATH, weights_path])\n","\n","def get_cfg(cfg_path):\n","    \"\"\"\n","    Returns model.cfg file path\n","    \"\"\"\n","    return os.path.sep.join([PATH, cfg_path])\n","\n","def load_model(cfg_path, weights_path):\n","    \"\"\"\n","    Returns loaded Darknet model\n","    \"\"\"\n","    print(\"- Loading model\")\n","    return cv2.dnn.readNetFromDarknet(cfg_path, weights_path)\n","\n","def show_dif(img, crop):\n","    _, ax = plt.subplots(1, 2, figsize=(12, 5))\n","    ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","    ax[0].set_title('Predicted image')\n","    ax[1].imshow(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n","    ax[1].set_title('Cropped image')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-p0MjzgmIJV","colab_type":"code","colab":{}},"source":["def get_pred(image, model):\n","    \"\"\"\n","    Returns prediction\n","    \"\"\"\n","    (H, W) = image.shape[:2]\n","    layer_names = model.getLayerNames()\n","    layer_names = [\n","        layer_names[i[0] - 1] for i in model.getUnconnectedOutLayers()\n","    ]\n","\n","    blob = cv2.dnn.blobFromImage(\n","        image,\n","        1 / 255.0,\n","        (416, 416),\n","        swapRB=True,\n","        crop=False\n","    )\n","\n","    model.setInput(blob)\n","    layer_outputs = model.forward(layer_names)\n","\n","    boxes = []\n","    confidences = []\n","    classes_ids = []\n","\n","    for output in tqdm(layer_outputs, desc=\"- Making prediction\"):\n","\n","        for detected in output:\n","\n","            scores = detected[5:]\n","            class_id = np.argmax(scores)\n","            conf = scores[class_id]\n","\n","            if conf > CONFTHRESH:\n","                box = detected[0:4] * np.array([W, H, W, H])\n","                (x_center, y_center, width, height) = box.astype(\"int\")\n","\n","                x = int(x_center - (width / 2))\n","                y = int(y_center - (height / 2))\n","\n","                boxes.append(\n","                    [x, y, int(width), int(height)]\n","                )\n","                confidences.append(float(conf))\n","                classes_ids.append(class_id)\n","\n","    idxs = cv2.dnn.NMSBoxes(\n","        boxes,\n","        confidences,\n","        CONFTHRESH,\n","        NMSTHRESH\n","    )\n","\n","    if len(idxs) > 0:\n","\n","        for i in idxs.flatten():\n","\n","            (x, y) = (boxes[i][0], boxes[i][1])\n","            (w, h) = (boxes[i][2], boxes[i][3])\n","            cropped_image = image[y:y+h, x:x+w]\n","\n","    return cropped_image\n","\n","\n","def main(cfg, weights, filename, output, batch=False):\n","\n","    cfg = get_cfg(cfg)\n","    weights = get_weights(weights)\n","    model = load_model(cfg, weights)\n","    \n","    if batch:\n","        for i in glob.glob(os.path.join(filename, '*.jpg')):\n","            image = cv2.imread(i, cv2.IMREAD_UNCHANGED)\n","\n","            start = time.time()\n","            output_image = get_pred(\n","                image=image,\n","                model=model,\n","            )\n","            print(f\" Predicted {i} in {time.time() - start:.2f} seconds\")\n","            out_img_path = os.path.join(\n","                output,\n","                os.path.basename(i)\n","            )\n","            output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n","            cv2.imwrite(out_img_path, output_image)\n","            print(f\"- Saved predicted image to {out_img_path}\")\n","            show_dif(image, output_image)\n","    else:\n","        image = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n","\n","        start = time.time()\n","        output_image = get_pred(\n","            image=image,\n","            model=model,\n","        )\n","        print(f\"- Predicted {filename} in {time.time() - start:.2f} seconds\")\n","        out_img_path = os.path.join(\n","            output,\n","            os.path.basename(filename)\n","        )\n","        output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n","        cv2.imwrite(out_img_path, output_image)\n","        print(f\"- Saved predicted image to {out_img_path}\")\n","        show_dif(image, output_image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdM4kJdWmKsz","colab_type":"code","colab":{}},"source":["main(\n","    cfg='cfg/custom/tiny-armoire.cfg',\n","    weights='backup/tiny-armoire_last.weights',\n","    filename='samples',\n","    output='predictions/',\n","    batch=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QU5copH7mOxe","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"drive_detect.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1K9sOnDGdr4_0I4K_yetwFZQhEMJkPaCp","authorship_tag":"ABX9TyPMs23JbiXPqHlJut5yXfBQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}