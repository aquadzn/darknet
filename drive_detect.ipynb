{"cells":[{"cell_type":"code","metadata":{"id":"ltBAm-vu2T9C","colab_type":"code","outputId":"2a042587-da7f-4183-90cb-1e11d1e048ae","executionInfo":{"status":"ok","timestamp":1587638468977,"user_tz":-120,"elapsed":738,"user":{"displayName":"William JACQUES","photoUrl":"","userId":"16955981879199111082"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%cd \"drive/My Drive/darknet\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/darknet\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oaTDH91U2ctY","colab_type":"code","outputId":"39f29915-47db-4a03-b319-7a0ee7fd9cbf","executionInfo":{"status":"ok","timestamp":1587638471845,"user_tz":-120,"elapsed":2318,"user":{"displayName":"William JACQUES","photoUrl":"","userId":"16955981879199111082"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["%ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34m3rdparty\u001b[0m/  chart.png               darknet.py           \u001b[01;34minclude\u001b[0m/      \u001b[01;34msamples\u001b[0m/\n","\u001b[01;34mbackup\u001b[0m/    chart_tiny-armoire.png  \u001b[01;34mdata\u001b[0m/                Makefile      \u001b[01;34mscripts\u001b[0m/\n","\u001b[01;34mbuild\u001b[0m/     \u001b[01;34mcmake\u001b[0m/                  Dockerfile           \u001b[01;34mmodels\u001b[0m/       \u001b[01;34msrc\u001b[0m/\n","build.ps1  CMakeLists.txt          drive_darknet.ipynb  \u001b[01;34mobj\u001b[0m/          train.sh\n","build.sh   darknet                 drive_detect.ipynb   \u001b[01;34mpredictions\u001b[0m/  \u001b[01;34mweights\u001b[0m/\n","\u001b[01;34mcfg\u001b[0m/       DarknetConfig.cmake.in  drive_scripts.ipynb  README.md\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yj3POo_pVs25","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"1b6eb65a-8d3c-43f4-eed5-cbe4750e9a39","executionInfo":{"status":"ok","timestamp":1587638473467,"user_tz":-120,"elapsed":3033,"user":{"displayName":"William JACQUES","photoUrl":"","userId":"16955981879199111082"}}},"source":["%ls backup"],"execution_count":3,"outputs":[{"output_type":"stream","text":["tiny-armoire_1000.weights  tiny-armoire_best.weights\n","tiny-armoire_2000.weights  tiny-armoire_last.weights\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_8OdQ0x-mG9g","colab_type":"text"},"source":["# Detect and save cropped image"]},{"cell_type":"markdown","metadata":{"id":"GmJqxdhe3j7y","colab_type":"text"},"source":["#### Utils:"]},{"cell_type":"code","metadata":{"id":"cPliO6vh2sAf","colab_type":"code","colab":{}},"source":["!pip install -q numpy matplotlib tqdm opencv-python"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXslsB5CEWY_","colab_type":"code","colab":{}},"source":["import os\n","import glob\n","import time\n","from typing import List\n","\n","import cv2\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","\n","\n","class Cropper:\n","    \"\"\"\n","    Load model from config and weights file\n","\n","    cfg:\n","        - Path to config file\n","    weights:\n","        - Path to weights file\n","    thresholds:\n","        - [0] is default threshold and [1] threshold used\n","        if nothing is detected with the first\n","    \"\"\"\n","    def __init__(self, cfg: str, weights: str, thresholds: List[float] = [0.5, 0.4]):\n","        self.cfg = cfg\n","        self.weights = weights\n","        self.CONFTHRESH = thresholds  # set to 0.3 if not working\n","        self.NMSTHRESH = 0.3\n","        self.model = cv2.dnn.readNetFromDarknet(self.cfg, self.weights)\n","        print(\"üöÄ Model loaded\")\n","\n","    \n","    def __show_dif(self, img, crop):\n","        \"\"\"\n","        Returns subplots with original and cropped images\n","        \"\"\"\n","        _, ax = plt.subplots(1, 2, figsize=(8, 4))\n","        ax[0].set_title('Predicted image')\n","        ax[0].imshow(img)\n","        ax[1].set_title('Cropped image')\n","        ax[1].imshow(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n","        plt.show()\n","\n","    def __crop(self, img):\n","        \"\"\"\n","        Get prediction and return cropped image\n","        \"\"\"\n","        (H, W) = img.shape[:2]\n","        layer_names = self.model.getLayerNames()\n","        layer_names = [\n","            layer_names[i[0] - 1] for i in self.model.getUnconnectedOutLayers()\n","        ]\n","\n","        blob = cv2.dnn.blobFromImage(\n","            img,\n","            1 / 255.0,\n","            (416, 416),\n","            swapRB=False,\n","            crop=False\n","        )\n","\n","        self.model.setInput(blob)\n","        layer_outputs = self.model.forward(layer_names)\n","\n","        boxes = []\n","        confidences = []\n","        classes_ids = []\n","        thresh = 0\n","\n","        for output in layer_outputs:\n","\n","            for detected in output:\n","\n","                scores = detected[5:]\n","                class_id = np.argmax(scores)\n","                conf = scores[class_id]\n","                if conf > self.CONFTHRESH[0]:\n","                    box = detected[0:4] * np.array([W, H, W, H])\n","                    (x_center, y_center, width, height) = box.astype(\"int\")\n","\n","                    x = int(x_center - (width / 2))\n","                    y = int(y_center - (height / 2))\n","\n","                    boxes.append(\n","                        [x, y, int(width), int(height)]\n","                    )\n","                    confidences.append(float(conf))\n","                    classes_ids.append(class_id)\n","                    thresh = self.CONFTHRESH[0]\n","\n","                elif self.CONFTHRESH[1] < conf < self.CONFTHRESH[0]:\n","                    box = detected[0:4] * np.array([W, H, W, H])\n","                    (x_center, y_center, width, height) = box.astype(\"int\")\n","\n","                    x = int(x_center - (width / 2))\n","                    y = int(y_center - (height / 2))\n","\n","                    boxes.append(\n","                        [x, y, int(width), int(height)]\n","                    )\n","                    confidences.append(float(conf))\n","                    classes_ids.append(class_id)\n","                    thresh = self.CONFTHRESH[1]\n","\n","                else:\n","                    cropped_image = None\n","\n","        print(f\"\\nüî¢ Threshold used: {thresh}\")\n","        idxs = cv2.dnn.NMSBoxes(\n","            boxes,\n","            confidences,\n","            thresh,\n","            self.NMSTHRESH\n","        )\n","\n","        if len(idxs) > 0:\n","\n","            for i in idxs.flatten():\n","\n","                (x, y) = (max(0, boxes[i][0]), max(0, boxes[i][1]))\n","                (w, h) = (boxes[i][2], boxes[i][3])\n","                cropped_image = img[y:y+h, x:x+w]\n","\n","        return cropped_image\n","\n","\n","    def predict(self, item: str, length: int = -1, output_folder: str = 'predictions/'):\n","        \"\"\"\n","        Predict one image or a folder of images and save cropped result\n","\n","        item:\n","            - Path to image or folder of images\n","        length:\n","            - Select *length* images from folder [default: -1]\n","        output_folder:\n","            - Folder to save result [default: 'predictions/']\n","        \"\"\"\n","        if not os.path.exists(output_folder):\n","            os.makedirs(output_folder)\n","\n","        if '.jpg' in item:\n","\n","            image = plt.imread(item)\n","            # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            print(\"üí° Making prediction\")\n","            start = time.time()\n","            output_image = self.__crop(img=image)\n","            if output_image is not None:\n","                print(f\"‚è±Ô∏è Predicted in {time.time() - start:.2f} sec\")\n","\n","                output_path = os.path.join(output_folder, os.path.basename(item))\n","\n","                output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n","\n","                cv2.imwrite(output_path, output_image)\n","\n","                print(f\"üíæ Saved to {output_path}\")\n","                self.__show_dif(image, output_image)\n","            else:\n","                print(f\"üò• Found nothing on {item}\")\n","\n","        else:\n","            not_detected = []\n","            items = glob.glob(\n","                os.path.join(item, '*.jpg')\n","            )[:length] if length > 0 else glob.glob(os.path.join(item, '*.jpg'))\n","            preds = [\n","                os.path.basename(i) for i in glob.glob(\n","                    os.path.join(output_folder, '*.jpg')\n","                )\n","            ]\n","\n","            for i in tqdm(items, desc=\"üí° Making predictions\"):\n","                if os.path.basename(i) not in preds: \n","                    image = plt.imread(i)\n","                    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","                    start = time.time()\n","                    output_image = self.__crop(img=image)\n","                    if output_image is not None:\n","                        print(f\"‚è±Ô∏è Predicted in {time.time() - start:.2f} sec\")\n","\n","                        output_path = os.path.join(output_folder, os.path.basename(i))\n","\n","                        output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n","\n","                        cv2.imwrite(output_path, output_image)\n","\n","                        print(f\"üíæ Saved to {output_path}\")\n","                        self.__show_dif(image, output_image)\n","                    else:\n","                        print(f\"üò• Found nothing on {i}\")\n","                        not_detected.append(i)\n","                else:\n","                    pass\n","            print(f\"\\n‚ùå There are {len(not_detected)} images where nothing was detected:\")\n","            for nd in not_detected:\n","                print('\\t' + nd)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWN8MrVE3jEJ","colab_type":"text"},"source":["#### Run:"]},{"cell_type":"code","metadata":{"id":"yBdecRcMJqKy","colab_type":"code","colab":{}},"source":["del crop"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdM4kJdWmKsz","colab_type":"code","outputId":"3fbd19b5-4008-4d35-908e-5bf833811b74","executionInfo":{"status":"ok","timestamp":1587644273369,"user_tz":-120,"elapsed":350,"user":{"displayName":"William JACQUES","photoUrl":"","userId":"16955981879199111082"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["crop = Cropper(\n","    cfg='cfg/custom/tiny-armoire.cfg',\n","    weights='backup/tiny-armoire_best.weights',\n","    thresholds=[0.5, 0.3]\n",")"],"execution_count":236,"outputs":[{"output_type":"stream","text":["üöÄ Model loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bCu4I0n_aWNw","colab_type":"code","colab":{}},"source":["crop.predict(\"data/obj/\", length=-1, output_folder=\"predictions/\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tEg-KxDYFfS","colab_type":"code","colab":{}},"source":["# !rm predictions/*.jpg"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LMu8g1JrEf-H","colab_type":"text"},"source":["not detected: 20200120_160657.jpg & C_05.jpg"]},{"cell_type":"code","metadata":{"id":"7amx-BFxEi_W","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}],"metadata":{"colab":{"name":"drive_detect.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"12RvXFHCYzaMRr2osbUP6cq1402EW2Tcw","authorship_tag":"ABX9TyM7sZAjkye+2h9Bb365aFfo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}