{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"drive_detect.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"12RvXFHCYzaMRr2osbUP6cq1402EW2Tcw","authorship_tag":"ABX9TyPC+/hIZ86ZY/cPlgix5suT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ltBAm-vu2T9C","colab_type":"code","colab":{}},"source":["%cd \"drive/My Drive/darknet\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oaTDH91U2ctY","colab_type":"code","colab":{}},"source":["%ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yj3POo_pVs25","colab_type":"code","colab":{}},"source":["%ls backup"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8OdQ0x-mG9g","colab_type":"text"},"source":["# Cropper armoire centrale"]},{"cell_type":"markdown","metadata":{"id":"GmJqxdhe3j7y","colab_type":"text"},"source":["#### Utils:"]},{"cell_type":"code","metadata":{"id":"cPliO6vh2sAf","colab_type":"code","colab":{}},"source":["!pip install -q numpy matplotlib tqdm opencv-python"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXslsB5CEWY_","colab_type":"code","colab":{}},"source":["import os\n","import glob\n","import time\n","from typing import List\n","\n","import cv2\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","\n","\n","class Cropper:\n","    \"\"\"\n","    Load model from config and weights file\n","\n","    cfg:\n","        - Path to config file\n","    weights:\n","        - Path to weights file\n","    thresholds:\n","        - [0] is default threshold and [1] threshold used\n","        if nothing is detected with the first\n","    \"\"\"\n","    def __init__(self, cfg: str, weights: str, thresholds: List[float] = [0.5, 0.4]):\n","        self.cfg = cfg\n","        self.weights = weights\n","        self.CONFTHRESH = thresholds  # set to 0.3 if not working\n","        self.NMSTHRESH = 0.3\n","        self.model = cv2.dnn.readNetFromDarknet(self.cfg, self.weights)\n","        print(\"üöÄ Model loaded\")\n","\n","    \n","    def __show_dif(self, img, crop):\n","        \"\"\"\n","        Returns subplots with original and cropped images\n","        \"\"\"\n","        _, ax = plt.subplots(1, 2, figsize=(8, 4))\n","        ax[0].set_title('Predicted image')\n","        ax[0].imshow(img)\n","        ax[1].set_title('Cropped image')\n","        ax[1].imshow(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n","        plt.show()\n","\n","    def __crop(self, img):\n","        \"\"\"\n","        Get prediction and return cropped image\n","        \"\"\"\n","        (H, W) = img.shape[:2]\n","        layer_names = self.model.getLayerNames()\n","        layer_names = [\n","            layer_names[i[0] - 1] for i in self.model.getUnconnectedOutLayers()\n","        ]\n","\n","        blob = cv2.dnn.blobFromImage(\n","            img,\n","            1 / 255.0,\n","            (416, 416),\n","            swapRB=False,\n","            crop=False\n","        )\n","\n","        self.model.setInput(blob)\n","        layer_outputs = self.model.forward(layer_names)\n","\n","        boxes = []\n","        confidences = []\n","        classes_ids = []\n","        thresh = 0\n","\n","        for output in layer_outputs:\n","\n","            for detected in output:\n","\n","                scores = detected[5:]\n","                class_id = np.argmax(scores)\n","                conf = scores[class_id]\n","                if conf > self.CONFTHRESH[0]:\n","                    box = detected[0:4] * np.array([W, H, W, H])\n","                    (x_center, y_center, width, height) = box.astype(\"int\")\n","\n","                    x = int(x_center - (width / 2))\n","                    y = int(y_center - (height / 2))\n","\n","                    boxes.append(\n","                        [x, y, int(width), int(height)]\n","                    )\n","                    confidences.append(float(conf))\n","                    classes_ids.append(class_id)\n","                    thresh = self.CONFTHRESH[0]\n","\n","                elif self.CONFTHRESH[1] < conf < self.CONFTHRESH[0]:\n","                    box = detected[0:4] * np.array([W, H, W, H])\n","                    (x_center, y_center, width, height) = box.astype(\"int\")\n","\n","                    x = int(x_center - (width / 2))\n","                    y = int(y_center - (height / 2))\n","\n","                    boxes.append(\n","                        [x, y, int(width), int(height)]\n","                    )\n","                    confidences.append(float(conf))\n","                    classes_ids.append(class_id)\n","                    thresh = self.CONFTHRESH[1]\n","\n","                else:\n","                    cropped_image = None\n","\n","        print(f\"\\nüî¢ Threshold used: {thresh}\")\n","        idxs = cv2.dnn.NMSBoxes(\n","            boxes,\n","            confidences,\n","            thresh,\n","            self.NMSTHRESH\n","        )\n","\n","        if len(idxs) > 0:\n","\n","            for i in idxs.flatten():\n","\n","                (x, y) = (max(0, boxes[i][0]), max(0, boxes[i][1]))\n","                (w, h) = (boxes[i][2], boxes[i][3])\n","                cropped_image = img[y:y+h, x:x+w]\n","\n","        return cropped_image\n","\n","\n","    def predict(self, item: str, length: int = -1, output_folder: str = 'predictions/'):\n","        \"\"\"\n","        Predict one image or a folder of images and save cropped result\n","\n","        item:\n","            - Path to image or folder of images\n","        length:\n","            - Select *length* images from folder [default: -1]\n","        output_folder:\n","            - Folder to save result [default: 'predictions/']\n","        \"\"\"\n","        if not os.path.exists(output_folder):\n","            os.makedirs(output_folder)\n","\n","        if '.jpg' in item:\n","\n","            image = plt.imread(item)\n","            # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            print(\"üí° Making prediction\")\n","            start = time.time()\n","            output_image = self.__crop(img=image)\n","            if output_image is not None:\n","                print(f\"‚è±Ô∏è Predicted in {time.time() - start:.2f} sec\")\n","\n","                output_path = os.path.join(output_folder, os.path.basename(item))\n","\n","                output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n","\n","                cv2.imwrite(output_path, output_image)\n","\n","                print(f\"üíæ Saved to {output_path}\")\n","                self.__show_dif(image, output_image)\n","            else:\n","                print(f\"üò• Found nothing on {item}\")\n","\n","        else:\n","            not_detected = []\n","            items = glob.glob(\n","                os.path.join(item, '*.jpg')\n","            )[:length] if length > 0 else glob.glob(os.path.join(item, '*.jpg'))\n","            preds = [\n","                os.path.basename(i) for i in glob.glob(\n","                    os.path.join(output_folder, '*.jpg')\n","                )\n","            ]\n","\n","            for i in tqdm(items, desc=\"üí° Making predictions\"):\n","                if os.path.basename(i) not in preds: \n","                    image = plt.imread(i)\n","                    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","                    start = time.time()\n","                    output_image = self.__crop(img=image)\n","                    if output_image is not None:\n","                        print(f\"‚è±Ô∏è Predicted in {time.time() - start:.2f} sec\")\n","\n","                        output_path = os.path.join(output_folder, os.path.basename(i))\n","\n","                        output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n","\n","                        cv2.imwrite(output_path, output_image)\n","\n","                        print(f\"üíæ Saved to {output_path}\")\n","                        self.__show_dif(image, output_image)\n","                    else:\n","                        print(f\"üò• Found nothing on {i}\")\n","                        not_detected.append(i)\n","                else:\n","                    pass\n","            print(f\"\\n‚ùå There are {len(not_detected)} images where nothing was detected:\")\n","            for nd in not_detected:\n","                print('\\t' + nd)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWN8MrVE3jEJ","colab_type":"text"},"source":["#### Run:"]},{"cell_type":"code","metadata":{"id":"yBdecRcMJqKy","colab_type":"code","colab":{}},"source":["del crop"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdM4kJdWmKsz","colab_type":"code","colab":{}},"source":["crop = Cropper(\n","    cfg='cfg/custom/tiny-armoire.cfg',\n","    weights='backup/tiny-armoire_best.weights',\n","    thresholds=[0.5, 0.3]\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCu4I0n_aWNw","colab_type":"code","colab":{}},"source":["crop.predict(\"../test/\", output_folder='../predictions')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tEg-KxDYFfS","colab_type":"code","colab":{}},"source":["# !rm predictions/*.jpg"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LMu8g1JrEf-H","colab_type":"text"},"source":["not detected: 20200120_160657.jpg & C_05.jpg"]},{"cell_type":"markdown","metadata":{"id":"7amx-BFxEi_W","colab_type":"text"},"source":["# Cropper en trois zones: gauche - milieu - droite"]},{"cell_type":"markdown","metadata":{"id":"bMSzGqt3jt3U","colab_type":"text"},"source":["#### Utils:"]},{"cell_type":"code","metadata":{"id":"JHpzB1TGjys0","colab_type":"code","colab":{}},"source":["import os\n","import glob\n","\n","import cv2\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","\n","\n","def split_images(input_folder: str, output_folder: str = './', method: str = 'plt'):\n","    \"\"\"\n","    Split folder of images in three parts\n","\n","    input_folder:\n","        - Path to images folder\n","    output_folder:\n","        - Path to output folder\n","    method:\n","        - Using plt or cv2 method\n","    \"\"\"\n","\n","    imgs = glob.glob(os.path.join(input_folder, '*.jpg'))\n","\n","    if method == 'plt':\n","        for i in tqdm(imgs, desc=\"‚úÇÔ∏è Splitting images\"):\n","            img = plt.imread(i)\n","            h, w, d = img.shape\n","            plt.imsave(\n","                f\"{os.path.join(output_folder, 'a/')}{os.path.basename(i)}\",\n","                img[:, :round(w*0.4)]\n","            )\n","            plt.imsave(\n","                f\"{os.path.join(output_folder, 'b/')}{os.path.basename(i)}\",\n","                img[:, round(w*0.4):round(w*0.6)]\n","            )\n","            plt.imsave(\n","                f\"{os.path.join(output_folder, 'c/')}{os.path.basename(i)}\",\n","                img[:, round(w*0.6):]\n","            )\n","\n","    elif method == 'cv2':\n","        for i in tqdm(imgs, desc=\"‚úÇÔ∏è Splitting images\"):\n","            img = cv2.imread(i)\n","            h, w, d = img.shape\n","            cv2.imwrite(\n","                f\"{os.path.join(output_folder, 'a/')}{os.path.basename(i)}\",\n","                img[:, :round(w*0.4)]\n","            )\n","            cv2.imwrite(\n","                f\"{os.path.join(output_folder, 'b/')}{os.path.basename(i)}\",\n","                img[:, round(w*0.4):round(w*0.6)]\n","            )\n","            cv2.imwrite(\n","                f\"{os.path.join(output_folder, 'c/')}{os.path.basename(i)}\",\n","                img[:, round(w*0.6):]\n","            )\n","    \n","    else:\n","        print(\"‚ùå Choose between 'plt' or 'cv2' for method\")\n","        return\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IcVZOA8VjqmP","colab_type":"text"},"source":["#### Run:"]},{"cell_type":"code","metadata":{"id":"9XUZAYCun2AO","colab_type":"code","colab":{}},"source":["split_images(input_folder='../predictions/', output_folder='../predictions/', method='plt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yB7NoXOijxsN","colab_type":"code","colab":{}},"source":["%ls ../predictions"],"execution_count":0,"outputs":[]}]}